= AI For Customer Experiences: A Retail Example
include::_graphacademy_llm.adoc[]
:slug: graphrag-customer-experience
:author: Zach Blumenfeld
:category: genai-tutorials
:tags: 
:neo4j-versions: 5.x
:page-pagination:
:page-product: graphrag-customer-experience
//:imagesdir: https://dev.assets.neo4j.com/wp-content/uploads/2024/

//image::https://dist.neo4j.com/wp-content/uploads/20240618104511/build-kg-genai-e1718732751482.png[width=800, align=center,link="https://llm-graph-builder.neo4jlabs.com/",window="_blank"]

Use GenAI + GraphRAG to improve customer experiences throughout multiple touch-points in the journey:

* *Discovery:* generating personalized marketing and email content
* *Search:* tailored semantic search to improve conversion
* *Recommendations:* targeted product recommendations
* *Support:* compliant AI scripts for customer support

This guide walks through setting up a full-stack GraphRAG application demonstrating all the above using Neo4j, LangChain (with LangServ), and OpenAI. The app focuses on a retail example using the https://github.com/neo4j-product-examples/graphrag-customer-experience#:~:text=H%26M%20Personalized%20Fashion%20Recommendations%20Dataset[H&M Personalized Fashion Recommendations Dataset^], a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc. All code can be found in the https://github.com/neo4j-product-examples/graphrag-customer-experience[GitHub repository^].

image::ai-cust-exp-architecture.png[align=center]

== Running The App

=== Prerequisites

1. https://docs.docker.com/engine/install/[Docker^]
2. https://platform.openai.com/docs/quickstart/account-setup[OpenAI API Key^]

=== Setup
Clone the repository
[source, bash]
----
git clone https://github.com/neo4j-product-examples/graphrag-customer-experience.git
----

create  a `.env` file with the below. Fill in your OpenAI key. You can use our pre-loaded retail demo database to start.
The git repository has directions for creating the database from source data if you are interested.

[source, bash]
----
#Neo4j Database
NEO4J_URI=neo4j+s://demo.neo4jlabs.com
NEO4J_USERNAME=retail
NEO4J_PASSWORD=retail
NEO4J_DATABASE=retail

#OpenAI
OPENAI_API_KEY=sk-...

#Other
# used by UI to navigate between pages.
# Only change for
ADVERTISED_ADDRESS="http://localhost"
----

=== Run
To start the app, run the following command:
[source, bash]
----
docker-compose up
----

To start and rebuild (after changing env variables or code), run

[source, bash]
----
docker-compose up --build
----

To stop the app, run

[source, bash]
----
docker-compose down
----

Open `http://localhost:8501` in your browser to interact with the app.

== How Each Page Works

To understand how each page works it helps to first under the graph data models used.  For Discovery, Search, and Recommendations we use the below model.

image::ai-cust-exp-product-data-model.png[width=600, align=center]

For support it resembles the below.

image::ai-cust-exp-support-data-model.png[width=600, align=center]

//TODO: Link for data loading
Directions & details for data loading can be found https://github.com/neo4j-product-examples/graphrag-customer-experience?tab=readme-ov-file#populating-databases[here^]. It requires a combination of structured data loading, embedding, and named entity recognition (NER).


=== Search Page (TODO: Pull in Workshop Notebook example to show quality improvements with new retrieval pattern))
Once you understand how the search page works it will be easier to understand Discovery & Recommendation pages.
The Search page uses a LangChain retriever to first perform vector search on product nodes.  It will then perform re-ranking and filtering based off a graph traversal to personalize the response.  Below is a diagram of the flow.

image::ai-cust-exp-app-search-flow.png[align=center]

The graph traversal improves results by using shared purchase patterns to incorporate customer preferences. The logic is similar to that of collaborative filtering.  Essentially it examines which customers have purchased similar products to the current use, then looks at other products heavily purchased by that group and cross-references against the vector search candidates.

//TODO coded markdown example?

[source, cypher]
----
//Match products based on the search query vector
CALL db.index.vector.queryNodes('product_text_embeddings', $vectorTopK, $embedding)
YIELD node, score
WITH node AS product, score AS searchScore

//calculate how often purchases of above products show up in local purchase graph "the purchaseScore" and re-rank
OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)
    -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {customerId: $customerId})
WITH count(a) AS purchaseScore, product, searchScore

//get corresponding articles, format results and return
OPTIONAL MATCH (product)<-[:VARIANT_OF]-(a:Article)
RETURN (1+purchaseScore)*searchScore AS score,
    collect({colourGroup: a.colourGroupName, graphicalAppearance: a.graphicalAppearanceName,
    articleId:a.articleId}) AS articleVariants,
    product {.*, `text`: Null, `textEmbedding`: Null, id: Null} AS metadata
    ORDER BY score DESC LIMIT $resTopK
----

This is a post-filtering graph pattern.

=== Discovery
The Discovery page uses the same retrieval query as search, but in an LLM chain, where the results are provided to an LLM to generate an email given context from other parameters like season/time-of-year.  See diagram below.

image::ai-cust-exp-app-discovery-flow.png[align=center]


The chain itself looks this
[source, python]
----
content_chain = (
RunnableParallel(
{
 'context': (lambda x: (x['customer_interests'], x['customer_id'])) | RunnableLambda(retriever),
 'customerName': (lambda x: x['customer_name']),
 'customerInterests': (lambda x: x['customer_interests']),
 'timeofYear': (lambda x: x['time_of_year']),
})
| prompt
| llm
| StrOutputParser())
----

In essence the search context and id for the user is passed to the retriever for the same graph query, while other details like customer name, interests, and time of year are passed to the LLM to help make decisions about choosing what content to write in the email.

The LLM prompt template is used:

[source, python]
----
f"""
You are a personal assistant named Sally for a fashion, home, and beauty company called HRM.
write an email to {customerName}, one of your customers, to recommend and summarize products based on:
- the current season / time of year: {timeofYear}
- Their recent searches & interests: {customerInterests}

Please only mention the products listed in the context below. Do not come up with or add any new products to the list.
The below candidates are recommended based on the purchase patterns of other customers in the HRM database.
Select the best 4 to 5 product subset from the context that best match
the time of year: {timeofYear} and the customers interests.
Each product comes with an https `url` field.
Make sure to provide that https url with descriptive name text in markdown for each product.

# Context:
"""
----

=== Recommendations (TODO: Pull in TSNE Graph Embedding Explanations)
The Recommendations page uses a different type of retriever based on vector + graph embeddings.  The basic pattern is presented below.

image::ai-cust-exp-app-rec-flow.png[align=center]


We use Neo4j Graph Data Science (GDS) to create these embeddings. See code here The embeddings create common co-purchase clusters as seen below
//TODO: TSNE plot snapshot (from notebook - make a gif)

The Retrieval Query looks like this
//TODO coded markdown example?

And the LLM chain....

//TODO: code snippit
The LLM is given some creative authority to choose between and mix and match items based on its own language understanding and seasonality.
//TODO: prompt template code snippit

=== Customer Support

The customer support page follows a basic chatbot workflow with the addition of an extra knowledge graph retriever shown below.  The frontend holds conversation history. When a user asks a question, the question and conversation history is sent to a condense_question chain to summarize into a single prompt.  That is then sent to retrieve relevant documents and knowledge graph entities which, in turn, is added to a final prompt template to generate an LLM response and send back to chat.

image:ai-cust-exp-app-support-flow.png[align=center]

This page uses a different part of the graph then the others. This part contains text chunks from a document as well as various knowledge graph entities and relationships extracted from those text chunks using the https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/[LLM Knowledge Graph Builder^].

The extracted entities acts as facts or "rules" that the LLM should prioritize when responding.  This workflow is designed for scenarios where nuanced domain context and logic needs to be followed by an LLM (such as policies or specific business logic). Extracting this from documents and expressing as entities and relationships in a knowledge graph more efficiently and accurately exposes this information to LLMs.





